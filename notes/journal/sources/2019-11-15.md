# 2019-11-15

Cette semaine j'ai surtout effectué de l'enseignement et de la correction de copies. J'ai aussi été confronté à `sympy` et la difficulté de lui faire effectuer des simplifications qui me semblaient évidentes. J'ai réussi à obtenir le *polynôme* de stabilité d'une méthode de Lawson basée sur RK(3,3) pour le problème suivant :
$$
  \partial_t U + AU + N(U)
$$
en linéarisant $N(U)$ avec une matrice $\dot{\Lambda}$ définie par :
$$
  \dot{\Lambda} = \begin{pmatrix}0&0&0 \\ 0&0&\mu \\ 0&0&\lambda \end{pmatrix}
$$

On obtient alors de manière un peu brute avec `sympy` :
$$
  U^{n+1} = \begin{bmatrix}1 & 0 & - \frac{\Delta t \mu \left(\Delta t^{2} \lambda^{2} e^{i \Delta t k v} \sin{\left(\frac{\Delta t}{2} + t^{n} \right)} + \Delta t \lambda e^{\frac{i \Delta t k v}{2}} \sin{\left(\Delta t + t^{n} \right)} + 2 \Delta t \lambda e^{i \Delta t k v} \sin{\left(\frac{\Delta t}{2} + t^{n} \right)} + e^{\frac{3 i \Delta t k v}{2}} \sin{\left(t^{n} \right)} + e^{\frac{i \Delta t k v}{2}} \sin{\left(\Delta t + t^{n} \right)} + 4 e^{i \Delta t k v} \sin{\left(\frac{\Delta t}{2} + t^{n} \right)}\right) e^{- i k v \left(\frac{3 \Delta t}{2} + t^{n}\right)}}{6}\\0 & 1 & \frac{\Delta t \mu \left(\Delta t^{2} \lambda^{2} e^{i \Delta t k v} \cos{\left(\frac{\Delta t}{2} + t^{n} \right)} + \Delta t \lambda e^{\frac{i \Delta t k v}{2}} \cos{\left(\Delta t + t^{n} \right)} + 2 \Delta t \lambda e^{i \Delta t k v} \cos{\left(\frac{\Delta t}{2} + t^{n} \right)} + e^{\frac{3 i \Delta t k v}{2}} \cos{\left(t^{n} \right)} + e^{\frac{i \Delta t k v}{2}} \cos{\left(\Delta t + t^{n} \right)} + 4 e^{i \Delta t k v} \cos{\left(\frac{\Delta t}{2} + t^{n} \right)}\right) e^{- i k v \left(\frac{3 \Delta t}{2} + t^{n}\right)}}{6}\\0 & 0 & \frac{\Delta t^{3} \lambda^{3}}{6} + \frac{\Delta t^{2} \lambda^{2}}{2} + \Delta t \lambda + 1\end{bmatrix}U^n
$$
Cette matrice est le fruit du *polynôme* en $\Delta t\dot{\Lambda}$ :
$$
  \frac{\Delta t^{3} e^{- \frac{\Delta t A}{2} - t^{n} A} \dot{\Lambda} e^{- \frac{\Delta t A}{2}} \dot{\Lambda} e^{\Delta t A} \dot{\Lambda} e^{t^{n} A}}{6} + \frac{\Delta t^{2} e^{- \Delta t A - t^{n} A} \dot{\Lambda} e^{\Delta t A} \dot{\Lambda} e^{t^{n} A}}{6} + \frac{\Delta t^{2} e^{- \frac{\Delta t A}{2} - t^{n} A} \dot{\Lambda} e^{- \frac{\Delta t A}{2}} \dot{\Lambda} e^{\Delta t A + t^{n} A}}{6} + \frac{\Delta t^{2} e^{- \frac{\Delta t A}{2} - t^{n} A} \dot{\Lambda} e^{\frac{\Delta t A}{2}} \dot{\Lambda} e^{t^{n} A}}{6} + \frac{\Delta t e^{- t^{n} A} \dot{\Lambda} e^{t^{n} A}}{6} + \frac{\Delta t e^{- \Delta t A - t^{n} A} \dot{\Lambda} e^{\Delta t A + t^{n} A}}{6} + \frac{2 \Delta t e^{- \frac{\Delta t A}{2} - t^{n} A} \dot{\Lambda} e^{\frac{\Delta t A}{2} + t^{n} A}}{3} + 1
$$
Ce qui est important c'est que uniquement la dernière colonne est remplie. On a à chaque fois quelque chose de homogène à une puissance de $\Delta t\lambda$ : $\Delta t^3 \lambda^2\mu$, $\Delta t^2 \lambda\mu$, $\Delta t\mu$. Et pour $\mu=0$ on retrouve bien la même matrice que dans le cas déjà traité précédemment.

J'ai passé du temps avec `sympy` pour conserver des cosinus et sinus dans la forme finale (ceux-ci étaient écrits en complexe sous leur forme polaire), et ainsi qu'écrire ceci de manière à pouvoir tester le plus simplement possible avec d'autres schémas. Puisque je ne sais pas trop quoi conclure avec cette matrice, je n'ai pas cherché encore à effectuer ce travail sur d'autres schémas.

Pour information, pour obtenir ce résultat, j'ai défini une fonction permettant de calcul $e^{tA}$, et je connais les instants $t$ où cette fonction sera évaluée dans le schéma (en prennant les notations d'un tableau de Butcher, c'est en $e^{\pm(t^n+c_i\Delta t)A}$. Puis je demande à `sympy` de me substituer toutes ces exponentielles de matrices et $\dot{\Lambda}$ simultanément pour obtenir ce résultat.

```{.py .numberLines .lineAnchors}
def m_exp(x):
  t = (x/A).simplify()
  M = sp.Matrix([[sp.cos(t),sp.sin(t),0],[-sp.sin(t),sp.cos(t),0],[0,0,sp.exp(-sp.I*k*v*t)]])
  return M
list_t = sum([[t,-t] for t in [tn,tn+dt,tn+dt/2,dt,dt/2]],[])
list_subs = [(sp.exp(t*A).expand(),m_exp(t*A)) for t in list_t]+[(L,mL)]

p.subs(list_subs,simultaneous=True) # p est notre polynôme en $\Delta t\dot{\Lambda}$
```

> J'ai pu testé avec un autre schéma RK(3,3) que le schéma RK(3,3) de Shu-Osher, défini par :
> $$
  \begin{aligned}
    u^{(1)} & = u^n + \frac{1}{2}\Delta t L(t^n,u^n) \\
    u^{(2)} & = 3u^n - 2u^{(1)} + 2\Delta t L(t^n+\frac{1}{2}\Delta t, u^{(1)}) \\
    u^{n+1} & = -\frac{1}{3}u^n + u^{(1)} + \frac{1}{3}u^{(2)} + \frac{1}{6}\Delta t L(t^n+\Delta t, u^{(2)})
  \end{aligned}
$$
> Et la fonction de stabilité du schéma de Lawson induit, avec une non-linéarité $\dot{\Lambda}$ (qui ne commute pas avec $e^{tA}$) est différente, et donc la matrice de stabilité varie aussi.

